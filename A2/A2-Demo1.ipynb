{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340b7142-7a1e-40dd-b2cb-53dc11d2c458",
   "metadata": {},
   "source": [
    "Tutorial: https://huggingface.co/docs/transformers/tasks/image_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a654217-3de0-49cf-9488-6a2c12d4767c",
   "metadata": {},
   "source": [
    "### Load Food-101 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a8fa10e-2a98-4ffe-b01a-ced3e6ba137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1b2a99d-32de-424c-8461-7971f29f816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd8f3038-0c9f-42a6-b72e-1b8f4b8c32e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c87183e0-7c58-4b41-a090-fcfb3ec9a3c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x384>,\n",
       " 'label': 81}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example - each example in the dataser has two fields: image and label\n",
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae8e781-3e43-4448-a41d-29538c274a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary that maps the label name to an integer and vice versa\n",
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5983c73e-4e12-44b9-a9a3-76ba32ce8415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prime_rib'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting label id to a label name:\n",
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f4a4b4-5e0e-45a4-8277-cb1fae19e22a",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9bb6bc26-d5fb-448b-a92f-d76ad61666e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a ViT image processor to process the image into a tensor:\n",
    "from transformers import AutoImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7a8573-2830-4b65-95f5-acc6225fbc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"google/vit-base-patch16-224-in21k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5cdcf30-b0b8-4b6a-a663-c6ab58c3d2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 20:53:52.628938: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5224df07-0ef0-46b8-9fa3-8ad4a8473c45",
   "metadata": {},
   "source": [
    "##### Pytorch\n",
    "\n",
    "Applying some image transformations to the images to make the model more robust against overfitting (with torchvisionâ€™s transforms module). We'll crop a random part of the image, resize it, and normalize it with the image mean and standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c95a9ab-247c-46dd-8741-e6a7e8d82dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c848202e-1334-46ea-b4c3-b0b8167e234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e87f8f63-9bc1-4e2c-977e-9fc89c0060ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85d010d1-b286-434b-9c3a-05a2f7fe64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ebdb0-fee7-4023-8cc7-6492949d2e7e",
   "metadata": {},
   "source": [
    "Then we create a preprocessing function to apply the transforms and return the pixel_values - the inputs to the model - of the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cba3effc-9e78-4766-b0a9-8d5c419b3e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f66ba8-6931-4dca-96f8-54a23c284fdd",
   "metadata": {},
   "source": [
    "To apply the preprocessing function over the entire dataset, use ðŸ¤— Datasets with_transform method. The transforms are applied on the fly when you load an element of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0011d6ad-6fd1-49a3-a242-ab5e09c96123",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8334cd-150c-4d50-b1ba-cc67ae0f4fb8",
   "metadata": {},
   "source": [
    "Now create a batch of examples using DefaultDataCollator. Unlike other data collators in ðŸ¤— Transformers, the DefaultDataCollator does not apply additional preprocessing such as padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c7ea520-5cc2-402f-9c42-39189fc1c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c33aa70-183f-45f9-8c51-016c9d197c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d65783d-8de2-4b24-835c-9b26b0d60816",
   "metadata": {},
   "source": [
    "##### TensorFlow\n",
    "\n",
    "To avoid overfitting and to make the model more robust, add some data augmentation to the training part of the dataset. Here we use Keras preprocessing layers to define the transformations for the training data (includes data augmentation), and transformations for the validation data (only center cropping, resizing and normalizing). You can use tf.imageor any other library you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f080171f-24b8-422f-a854-e4fb2fc12f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90d73db1-edd6-481a-8318-ce2286b6933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e2b6b0d-26ce-4cec-afbf-6b309698cf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-02 21:09:59.642683: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10533 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:03:00.0, compute capability: 6.1\n",
      "2023-12-02 21:09:59.643494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10534 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1\n",
      "2023-12-02 21:09:59.644053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10534 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:81:00.0, compute capability: 6.1\n",
      "2023-12-02 21:09:59.644613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10534 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:82:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "train_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c85cb4d-21f3-4ca3-9600-1184473f7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a974e6-3b2a-48e7-a3cb-943b120fe16d",
   "metadata": {},
   "source": [
    "Next, create functions to apply appropriate transformations to a batch of images, instead of one image at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee560829-031a-4d01-8cd6-bf462c97e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d38a1b4-ee12-4af9-8514-bc096cecf8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    # `expand_dims()` is used to add a batch dimension since\n",
    "    # the TF augmentation layers operates on batched inputs.\n",
    "    return tf.expand_dims(tf_image, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af41a7dd-6bed-42bc-b8e7-aca084051cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b5653b1-4947-4271-9219-2eb1d19c0c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced85c31-907f-4f12-a3a6-b09b089a3c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
